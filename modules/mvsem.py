import torch
import torch.nn as nn
import torch.nn.functional as F


class MVSEM(nn.Module):
    """
    Final MVSEM - ÁªìÂêà‰∫Ü‰ª•‰∏ãÁâπÊÄß:
    1. Memory Network (MHA Âæ™ÁéØ) Ê†∏ÂøÉÈÄªËæëÔºåÁ¨¶ÂêàÈ°∂ÂàäËÆ∫ÊñáËÆæËÆ°ÊÄùË∑Ø„ÄÇ
    2. ÂÆâÂÖ®ÁöÑ vocab_size ËÆ°ÁÆó (max index + 1)ÔºåËß£ÂÜ≥ IndexError„ÄÇ
    3. ÈùûÁ∫øÊÄßÊäïÂΩ±Â±Ç (512 -> 2048)ÔºåËß£ÂÜ≥ CCRA Áª¥Â∫¶‰∏çÂåπÈÖçÈóÆÈ¢ò„ÄÇ

    ËæìÂÖ•: reports_ids [B, T]
    ËæìÂá∫: MS [B, T, out_dim] (‰æãÂ¶Ç [B, T, 2048])
    """

    def __init__(self, args, tokenizer):
        super(MVSEM, self).__init__()

        # --- 1. Áª¥Â∫¶ÈÖçÁΩÆ ---
        d_model = getattr(args, "d_model", 512)#ËÆ∞ÂøÜÁΩëÁªúÂÜÖÈÉ®Â∑•‰ΩúÁöÑÁª¥Â∫¶ÔºàÁ±ª‰ºº Transformer ÈáåÈù¢ÁöÑ d_modelÔºâÔºåÂÖàÁî® 512„ÄÇ
        self.d_model = d_model
        self.out_dim = getattr(args, "d_vf", 2048)  # ÁõÆÊ†áËæìÂá∫Áª¥Â∫¶ (2048 for CCRA)
        self.num_memory = getattr(args, "num_memory", 8)#ËÆ∞ÂøÜÊßΩÁöÑ‰∏™Êï∞ÔºåÊØîÂ¶Ç 8 ‰∏™‚Äú‰æøÁ≠æ‚Äù„ÄÇ

        # --- 2. Ê†∏ÂøÉÁªÑ‰ª∂ÂàùÂßãÂåñ ---

        # ËØçË°®Â§ßÂ∞èÂà§Êñ≠ (üî¥ ÈááÁî®ÊúÄÂÆâÂÖ®ÁöÑÊñπÊ≥ï: max index + 1)ÈÄöËøá tokenizer ÁöÑ‰∏çÂêåÂ±ûÊÄßÂ∞ùËØïÊé®Êñ≠ vocab_sizeÔºö
        # Â¶ÇÊûúÊúâ vocab_sizeÔºåÁõ¥Êé•Áî®„ÄÇ # Âê¶ÂàôÁî® token2idx ÁöÑÊúÄÂ§ßÂÄºÂä† 1ÔºåÊàñ idx2token ÁöÑÊúÄÂ§ß/ÈïøÂ∫¶Êù•Êé®Êñ≠„ÄÇ
        # Ëøô‰∏™Á≠ñÁï•Á°Æ‰øù Embedding ÁöÑ vocab_size Ëá≥Â∞ëË¶ÜÁõñËæìÂÖ•‰∏≠ÁöÑÊúÄÂ§ß token Á¥¢ÂºïÔºåÈÅøÂÖç IndexError„ÄÇ
        if hasattr(tokenizer, "vocab_size"):
            vocab_size = tokenizer.vocab_size
        elif hasattr(tokenizer, "token2idx"):
            # ÊâæÂà∞Â≠óÂÖ∏‰∏≠ÊúÄÂ§ßÁöÑÁ¥¢ÂºïÂÄºÔºåÂπ∂ +1 Á°Æ‰øù Embedding Á©∫Èó¥Ë∂≥Â§ü
            vocab_size = max(tokenizer.token2idx.values()) + 1
        elif hasattr(tokenizer, "idx2token"):
            if isinstance(tokenizer.idx2token, dict):
                vocab_size = max(tokenizer.idx2token.keys()) + 1
            else:
                vocab_size = len(tokenizer.idx2token)
        else:
            raise ValueError(
                "Cannot infer vocab size from tokenizer; please check tokenizer attributes."
            )

        pad_id = getattr(tokenizer, "pad_token_id", 0) #Embedding Êó∂‰º†Áªô padding_idxÔºåÁ°Æ‰øùÂ°´ÂÖÖ token ‰∏çÂèÇ‰∏éÊ¢ØÂ∫¶Êõ¥Êñ∞Âπ∂Âú®‰º™Êé©Á†Å‰∏≠Â§ÑÁêÜ„ÄÇ

        # A. ËØçÂµåÂÖ•Â±Ç (ÂÜÖÈÉ® d_model Áª¥)
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)#ËæìÂÖ• reports_ids ÁöÑÂΩ¢Áä∂‰∏∫ [B, T]ÔºåÂµåÂÖ•ËæìÂá∫‰∏∫ [B, T, d_model]„ÄÇ

        # B. ÂàùÂßãËÆ∞ÂøÜÁü©Èòµ M0 (ÂèØÂ≠¶‰π†ÂèÇÊï∞)
        self.mem_init = nn.Parameter(torch.randn(self.num_memory, d_model))

        # C. ËÆ∞ÂøÜÊõ¥Êñ∞Áî®ÁöÑ Multi-Head Attention Â±Ç (Âú® d_model Áª¥Â∫¶‰∏äÊìç‰Ωú)
        self.mha = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=getattr(args, "num_heads", 8),
            batch_first=True
        )#‰ΩøÁî® embed_dim=d_modelÔºåÂ§¥Êï∞ÈªòËÆ§ 8„ÄÇbatch_first=True Ë°®Á§∫ËæìÂÖ•Ê†ºÂºè‰∏∫ [B, S, E]Ôºå‰∏éÂêéÁª≠‰ª£Á†Å‰øùÊåÅ‰∏ÄËá¥„ÄÇ
        self.ln = nn.LayerNorm(d_model)  # Áî®‰∫é MHA ÂêéÁöÑ LayerNorm

        # --- 3. ÈùûÁ∫øÊÄßÊäïÂΩ±Â±Ç (512 -> 2048 ÂØπÈΩê) ---
        if self.d_model != self.out_dim:
            # Non-linear Projection: Á¨¶ÂêàËÆ∫ÊñáË¶ÅÊ±ÇÁöÑÁâπÂæÅÂØπÈΩê
            self.projector = nn.Sequential(
                nn.Linear(self.d_model, self.out_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
        else:
            self.projector = None

    def forward(self, reports_ids):
        """
        reports_ids: [B, T]
        ËøîÂõû: MS: [B, T, out_dim]
        """
        device = reports_ids.device
        B, T = reports_ids.shape

        # 1. Token -> Embedding: [B, T, d_model]
        emb = self.embedding(reports_ids)

        # 2. ÂàùÂßãÂåñËÆ∞ÂøÜ M0 -> M_t: [B, N_M, d_model]
        M_t = self.mem_init.unsqueeze(0).expand(B, self.num_memory, self.d_model).to(device)

        mem_seq = []

        # 3. Âæ™ÁéØÊØè‰∏™Êó∂Èó¥Ê≠• tÔºåÊâßË°å Memory Update (ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÈÄªËæë)
        for t in range(T):
            # ÂΩìÂâçËØçÂêëÈáè y_t: [B, d_model]
            y_t = emb[:, t, :]

            # ÊãºÊé•: Key/Value = [Memory, Current_Word] -> [B, N_M+1, d_model]
            kv = torch.cat([M_t, y_t.unsqueeze(1)], dim=1)

            # MHA Êõ¥Êñ∞ËÆ∞ÂøÜÔºö
            M_new, _ = self.mha(query=M_t, key=kv, value=kv)

            # ÊÆãÂ∑ÆËøûÊé• + LayerNorm
            M_t = self.ln(M_t + M_new)

            # ÊëòË¶Å: ÂØπ N_M ‰∏™ËÆ∞ÂøÜÊßΩÂèñÂπ≥ÂùáÔºåÂæóÂà∞ÂΩìÂâçÊó∂Èó¥Ê≠•ÁöÑ‰∏ä‰∏ãÊñáÂêëÈáè M_t -> [B, d_model]
            mem_summary = M_t.mean(dim=1)
            mem_seq.append(mem_summary)

        # 4. Â†ÜÂè†ÂæóÂà∞ÂéüÂßãËÆ∞ÂøÜÊµÅ MS [B, T, d_model]
        MS = torch.stack(mem_seq, dim=1)

        # 5. Â∫îÁî®ÈùûÁ∫øÊÄßÊäïÂΩ± [B, T, d_model] -> [B, T, out_dim] (ÂØπÈΩêCCRA)
        if self.projector is not None:
            MS = self.projector(MS)

        return MS
