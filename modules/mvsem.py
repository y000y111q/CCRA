import torch
import torch.nn as nn
import torch.nn.functional as F


class MVSEM(nn.Module):
    """
    GRUç®€åŒ–ç‰ˆ MVSEMï¼ˆå¤šè§†å›¾è¯­ä¹‰åµŒå…¥æ¨¡å—ï¼‰ï¼š
    æ ¸å¿ƒé€»è¾‘ï¼šä½¿ç”¨ GRU æŒ‰ç…§æŠ¥å‘Š token åºåˆ—çš„æ—¶é—´æ­¥è¿›è¡Œè®°å¿†ç§¯ç´¯ã€‚
    åŒæ—¶é›†æˆäº† Index ä¿®å¤å’Œ Non-linear Projectionã€‚

    è¾“å…¥: reports_ids [B, T]
    è¾“å‡º: MS [B, T, out_dim] (ä¾‹å¦‚ [B, T, 2048])
    """

    def __init__(self, args, tokenizer):
        super(MVSEM, self).__init__()
        # --- 1. ç»´åº¦é…ç½® ---
        # å†…éƒ¨ç»´åº¦ d_modelï¼šç”¨äºŽ Embedding å’Œ GRU (é»˜è®¤ 512)
        d_model = getattr(args, "d_model", 512)
        self.d_model = d_model
        # è¾“å‡ºç»´åº¦ out_dimï¼šå¤–éƒ¨éœ€è¦çš„ç»´åº¦ (ä¾‹å¦‚ CCRA æœŸæœ›çš„ 2048 è§†è§‰ç‰¹å¾ç»´åº¦)
        self.out_dim = getattr(args, "d_vf", 2048)

        # --- 2. æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ---

        # ðŸ”´ è¯è¡¨å¤§å°åˆ¤æ–­ (é‡‡ç”¨æœ€å®‰å…¨çš„æ–¹æ³•: max index + 1ï¼Œä¿®å¤ IndexError)
        if hasattr(tokenizer, "vocab_size"):
            vocab_size = tokenizer.vocab_size
        elif hasattr(tokenizer, "token2idx"):
            # æ‰¾åˆ°æœ€å¤§çš„ç´¢å¼•å€¼ï¼Œå¹¶ +1 ç¡®ä¿ Embedding ç©ºé—´è¶³å¤Ÿ
            vocab_size = max(tokenizer.token2idx.values()) + 1
        elif hasattr(tokenizer, "idx2token"):
            if isinstance(tokenizer.idx2token, dict):
                vocab_size = max(tokenizer.idx2token.keys()) + 1
            else:
                vocab_size = len(tokenizer.idx2token)
        else:
            raise ValueError(
                "Cannot infer vocab size from tokenizer; please check tokenizer attributes."
            )

        pad_id = getattr(tokenizer, "pad_token_id", 0)
        # A. è¯åµŒå…¥å±‚ (å†…éƒ¨ d_model ç»´)
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)
        # B. GRU å•å…ƒ (å®žçŽ°æŒ‰æ—¶é—´æ­¥æ›´æ–°è®°å¿† M_t çš„åŠŸèƒ½)
        self.gru = nn.GRU(
            input_size=self.d_model,  # è¾“å…¥æ˜¯ d_model ç»´çš„ embedding
            hidden_size=self.d_model,  # éšè—çŠ¶æ€ M_t ä¹Ÿæ˜¯ d_model ç»´
            num_layers=1,
            batch_first=True  # ç¡®ä¿è¾“å…¥è¾“å‡ºæ˜¯ [B, T, D]
        )
        # --- 3. éžçº¿æ€§æŠ•å½±å±‚ (512 -> 2048 å¯¹é½) ---
        if self.d_model != self.out_dim:
            # Non-linear Projection: Linear -> ReLU -> Dropout
            self.projector = nn.Sequential(
                nn.Linear(self.d_model, self.out_dim),  # å°† d_model (512) æŠ•å°„åˆ° out_dim (2048)
                nn.ReLU(),  # éžçº¿æ€§æ¿€æ´»
                nn.Dropout(0.1)  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            )
        else:
            self.projector = None

    def forward(self, reports_ids):
        """
        reports_ids: [B, T]
        è¿”å›ž: MS: [B, T, out_dim] 
        """
        # 1. Token -> Embedding: [B, T, d_model]
        emb = self.embedding(reports_ids)
        # 2. GRU æ›´æ–°è®°å¿†ï¼š
        #    MS æ˜¯ GRU åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ M_tï¼Œå®ƒå°±æ˜¯è®ºæ–‡ä¸­è¯´çš„â€œä¸²èµ·æ¥çš„ Mtâ€
        MS, _ = self.gru(emb)  # MS: [B, T, d_model]
        # 3. åº”ç”¨éžçº¿æ€§æŠ•å½± [B, T, d_model] -> [B, T, out_dim] 
        if self.projector is not None:
            MS = self.projector(MS)
        return MS
